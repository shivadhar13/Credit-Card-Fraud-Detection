{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNlFKnqS0sNkuR6Jaiw7ls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivadhar13/Credit-Card-Fraud-Detection/blob/main/Credit%20Card%20Fraud%20Detection%20Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score)\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, IsolationForest\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "\n",
        "# ---------------------------\n",
        "# Config / Hyperparameters\n",
        "# ---------------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "DATA_PATH = \"creditcard.csv\"\n",
        "MAJORITY_DOWN_TO = 20000      # prototype speed-up: downsample majority to this many rows (set None to keep all)\n",
        "SMOTE_RANDOM_STATE = SEED\n",
        "\n",
        "PCA_COMPONENTS = 10           # smaller for fast prototype; increase to 20 for final run\n",
        "RF_ESTIMATORS = 100\n",
        "GB_ESTIMATORS = 100\n",
        "ISO_ESTIMATORS = 100\n",
        "\n",
        "AE_EPOCHS = 30                # fewer epochs for prototype\n",
        "AE_BATCH = 512\n",
        "\n",
        "STACKING_FOLDS = 3            # number of folds for OOF stacking (3 is faster; 5 gives better meta-features)\n",
        "HOLDOUT_RATIO = 0.20          # for threshold calibration\n",
        "\n",
        "OUTPUT_CSV = \"model_comparative_results_fast.csv\"\n",
        "OUTPUT_PLOT = \"model_accuracy_comparison_fast.png\"\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utility & pipeline functions\n",
        "# ---------------------------\n",
        "def load_data(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load dataset and assert basic expectations.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Dataset not found at {path}. Place 'creditcard.csv' in script folder.\")\n",
        "    df = pd.read_csv(path)\n",
        "    print(f\"[load_data] Loaded dataset with shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def preprocess(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Drop NA, scale Time and Amount, return X (features) and y (target).\"\"\"\n",
        "    df = df.dropna()\n",
        "    X = df.drop(columns=['Class'])\n",
        "    y = df['Class'].astype(int)\n",
        "    scaler = StandardScaler()\n",
        "    # Scale Amount and Time columns (other 28 PCA features are already numeric)\n",
        "    X_scaled = X.copy()\n",
        "    X_scaled['Amount'] = scaler.fit_transform(X[['Amount']])\n",
        "    X_scaled['Time'] = scaler.fit_transform(X[['Time']])\n",
        "    print(\"[preprocess] Completed standard scaling for 'Amount' and 'Time'.\")\n",
        "    return X_scaled, y\n",
        "\n",
        "\n",
        "def train_test_split_stratified(X: pd.DataFrame, y: pd.Series, test_size: float = 0.3, seed: int = SEED):\n",
        "    \"\"\"Stratified train-test split and print class distribution.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=seed)\n",
        "    print(f\"[split] Train shape: {X_train.shape}  Test shape: {X_test.shape}\")\n",
        "    print(\"[split] Train class distribution:\\n\", y_train.value_counts())\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def fast_smote_subset(X_train: pd.DataFrame, y_train: pd.Series, maj_down_to: int = MAJORITY_DOWN_TO,\n",
        "                      random_state: int = SMOTE_RANDOM_STATE) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "    if maj_down_to is not None:\n",
        "        majority_count = (y_train == 0).sum()\n",
        "        if majority_count > maj_down_to:\n",
        "            maj_idx = y_train[y_train == 0].sample(n=maj_down_to, random_state=random_state).index\n",
        "            min_idx = y_train[y_train == 1].index\n",
        "            subset_idx = maj_idx.union(min_idx)\n",
        "            X_train_small = X_train.loc[subset_idx].copy()\n",
        "            y_train_small = y_train.loc[subset_idx].copy()\n",
        "            print(f\"[fast_smote] Downsampled majority from {majority_count} to {maj_down_to} for prototyping.\")\n",
        "        else:\n",
        "            X_train_small = X_train.copy()\n",
        "            y_train_small = y_train.copy()\n",
        "    else:\n",
        "        X_train_small = X_train.copy()\n",
        "        y_train_small = y_train.copy()\n",
        "\n",
        "    sm = SMOTE(random_state=random_state)\n",
        "    X_res, y_res = sm.fit_resample(X_train_small, y_train_small)\n",
        "    print(f\"[fast_smote] After SMOTE: class counts = {np.bincount(y_res)}\")\n",
        "    return X_res, y_res, X_train_small, y_train_small\n",
        "\n",
        "\n",
        "def apply_pca(X_train_resampled: np.ndarray, X_test: np.ndarray, n_components: int = PCA_COMPONENTS):\n",
        "    \"\"\"Fit PCA on resampled training set and transform both train and test.\"\"\"\n",
        "    pca = PCA(n_components=n_components, random_state=SEED)\n",
        "    X_res_pca = pca.fit_transform(X_train_resampled)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    print(f\"[pca] Applied PCA: components={n_components}, explained_variance_ratio_sum={pca.explained_variance_ratio_.sum():.3f}\")\n",
        "    return pca, X_res_pca, X_test_pca\n",
        "\n",
        "\n",
        "def train_supervised_models(X_train_pca: np.ndarray, y_train_resampled: np.ndarray):\n",
        "    \"\"\"Train three supervised models quickly: RandomForest, LogisticRegression, GradientBoosting.\"\"\"\n",
        "    rf = RandomForestClassifier(n_estimators=RF_ESTIMATORS, random_state=SEED, n_jobs=-1)\n",
        "    lr = LogisticRegression(max_iter=1000, random_state=SEED)\n",
        "    gb = GradientBoostingClassifier(n_estimators=GB_ESTIMATORS, learning_rate=0.1, random_state=SEED)\n",
        "\n",
        "    print(\"[train_supervised] Training Random Forest, Logistic Regression, Gradient Boosting...\")\n",
        "    rf.fit(X_train_pca, y_train_resampled)\n",
        "    lr.fit(X_train_pca, y_train_resampled)\n",
        "    gb.fit(X_train_pca, y_train_resampled)\n",
        "    print(\"[train_supervised] Supervised models trained.\")\n",
        "    return rf, lr, gb\n",
        "\n",
        "\n",
        "def build_and_train_autoencoder(X_majority_pca: np.ndarray, input_dim: int):\n",
        "    \"\"\"Build a small dense autoencoder and train on the majority (non-fraud) PCA data.\"\"\"\n",
        "    encoding_dim = max(4, input_dim // 3)\n",
        "\n",
        "    ae = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(encoding_dim, activation='relu'),\n",
        "        layers.Dense(max(2, encoding_dim // 2), activation='relu'),\n",
        "        layers.Dense(encoding_dim, activation='relu'),\n",
        "        layers.Dense(input_dim, activation='linear'),  # reconstruct PCA features\n",
        "    ])\n",
        "    ae.compile(optimizer=optimizers.Adam(1e-3), loss='mse')\n",
        "    early = callbacks.EarlyStopping(monitor='loss', patience=4, restore_best_weights=True)\n",
        "\n",
        "    print(\"[autoencoder] Training AE on majority-class PCA data...\")\n",
        "    ae.fit(X_majority_pca, X_majority_pca, epochs=AE_EPOCHS, batch_size=AE_BATCH, callbacks=[early], verbose=1)\n",
        "    print(\"[autoencoder] Autoencoder training complete.\")\n",
        "    return ae\n",
        "\n",
        "\n",
        "def compute_anomaly_scores(ae_model, iso_model, X_pca: np.ndarray):\n",
        "    \"\"\"Compute combined anomaly score from AE reconstruction error and IsolationForest score.\"\"\"\n",
        "    # AE reconstruction error\n",
        "    recon = ae_model.predict(X_pca)\n",
        "    rec_err = np.mean(np.square(recon - X_pca), axis=1)\n",
        "    rec_err_norm = (rec_err - rec_err.min()) / (rec_err.max() - rec_err.min() + 1e-12)\n",
        "\n",
        "    # Isolation forest anomaly score (we invert decision_function so higher means more anomalous)\n",
        "    iso_score = -iso_model.decision_function(X_pca)\n",
        "    iso_score_norm = (iso_score - iso_score.min()) / (iso_score.max() - iso_score.min() + 1e-12)\n",
        "\n",
        "    combined = (rec_err_norm + iso_score_norm) / 2.0\n",
        "    return combined, rec_err_norm, iso_score_norm\n",
        "\n",
        "\n",
        "def build_oof_meta_features(X_res_pca: np.ndarray, y_res: np.ndarray, ae, iso, folds: int = STACKING_FOLDS):\n",
        "    \"\"\"\n",
        "    Create out-of-fold (OOF) meta-features for stacking:\n",
        "    OOF features: prob_rf, prob_lr, prob_gb, anomaly_score\n",
        "    \"\"\"\n",
        "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
        "    n = X_res_pca.shape[0]\n",
        "    oof_rf = np.zeros(n)\n",
        "    oof_lr = np.zeros(n)\n",
        "    oof_gb = np.zeros(n)\n",
        "    oof_anom = np.zeros(n)\n",
        "\n",
        "    print(f\"[oof] Building OOF meta-features using {folds}-fold CV (this trains small models {folds} times)...\")\n",
        "    for train_idx, val_idx in kf.split(X_res_pca, y_res):\n",
        "        X_tr, X_val = X_res_pca[train_idx], X_res_pca[val_idx]\n",
        "        y_tr, _ = y_res[train_idx], y_res[val_idx]\n",
        "\n",
        "        m_rf = RandomForestClassifier(n_estimators=RF_ESTIMATORS, random_state=SEED, n_jobs=-1)\n",
        "        m_lr = LogisticRegression(max_iter=1000, random_state=SEED)\n",
        "        m_gb = GradientBoostingClassifier(n_estimators=GB_ESTIMATORS, learning_rate=0.1, random_state=SEED)\n",
        "\n",
        "        m_rf.fit(X_tr, y_tr)\n",
        "        m_lr.fit(X_tr, y_tr)\n",
        "        m_gb.fit(X_tr, y_tr)\n",
        "\n",
        "        oof_rf[val_idx] = m_rf.predict_proba(X_val)[:, 1]\n",
        "        oof_lr[val_idx] = m_lr.predict_proba(X_val)[:, 1]\n",
        "        oof_gb[val_idx] = m_gb.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        # compute anomaly score for validation fold (AE + IsolationForest are trained on majority)\n",
        "        recon_val = ae.predict(X_val)\n",
        "        rec_err_val = np.mean(np.square(recon_val - X_val), axis=1)\n",
        "        rec_err_val_norm = (rec_err_val - rec_err_val.min()) / (rec_err_val.max() - rec_err_val.min() + 1e-12)\n",
        "\n",
        "        iso_score_val = -iso.decision_function(X_val)\n",
        "        iso_score_val_norm = (iso_score_val - iso_score_val.min()) / (iso_score_val.max() - iso_score_val.min() + 1e-12)\n",
        "\n",
        "        oof_anom[val_idx] = (rec_err_val_norm + iso_score_val_norm) / 2.0\n",
        "\n",
        "    meta_X = np.vstack([oof_rf, oof_lr, oof_gb, oof_anom]).T\n",
        "    print(\"[oof] OOF meta-features built.\")\n",
        "    return meta_X\n",
        "\n",
        "\n",
        "def calibrate_threshold(meta_X_train: np.ndarray, meta_y_train: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Holdout a small part of meta-train and search threshold that maximizes F1.\n",
        "    Returns the chosen threshold.\n",
        "    \"\"\"\n",
        "    X_hold, X_val, y_hold, y_val = train_test_split(meta_X_train, meta_y_train, test_size=HOLDOUT_RATIO,\n",
        "                                                    stratify=meta_y_train, random_state=SEED)\n",
        "    tmp = LogisticRegression(max_iter=1000, random_state=SEED)\n",
        "    tmp.fit(X_hold, y_hold)\n",
        "    probs = tmp.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    best_t = 0.5\n",
        "    best_f1 = -1\n",
        "    for t in np.linspace(0.01, 0.99, 99):\n",
        "        preds = (probs >= t).astype(int)\n",
        "        f1 = f1_score(y_val, preds)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "    print(f\"[threshold] Selected threshold {best_t:.2f} with validation F1 {best_f1:.4f}\")\n",
        "    return best_t\n",
        "\n",
        "\n",
        "def compute_metrics_table(y_true, model_pred_dict, model_score_dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute Accuracy, Precision, Recall, F1, ROC-AUC for each model.\n",
        "    model_pred_dict: {name: y_pred_labels}\n",
        "    model_score_dict: {name: score/probability used for ROC-AUC}\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for name, y_pred in model_pred_dict.items():\n",
        "        score = model_score_dict.get(name, y_pred)  # if no score provided, use labels\n",
        "        acc = accuracy_score(y_true, y_pred) * 100\n",
        "        prec = precision_score(y_true, y_pred, zero_division=0) * 100\n",
        "        rec = recall_score(y_true, y_pred) * 100\n",
        "        f1 = f1_score(y_true, y_pred) * 100\n",
        "        auc = roc_auc_score(y_true, score) * 100\n",
        "        results[name] = (acc, prec, rec, f1, auc)\n",
        "\n",
        "    res_df = pd.DataFrame(results, index=['Accuracy (%)', 'Precision (%)', 'Recall (%)', 'F1-Score (%)', 'ROC-AUC (%)']).T\n",
        "    return res_df\n",
        "\n",
        "\n",
        "def plot_accuracy_chart(res_df: pd.DataFrame, out_fname: str = OUTPUT_PLOT):\n",
        "    \"\"\"Plot a bar chart of accuracy values to visually match your figure style.\"\"\"\n",
        "    order = list(res_df.index)\n",
        "    accuracies = res_df['Accuracy (%)'].values\n",
        "\n",
        "    plt.figure(figsize=(10.5, 4.6), dpi=200)\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    colors = [\"#39A2DB\", \"#E45756\", \"#2CA02C\", \"#6B7B8C\", \"#E6BF48\", \"#C55A11\", \"#7C2F11\"][:len(order)]\n",
        "    bars = plt.bar(order, accuracies, color=colors, edgecolor='none')\n",
        "    plt.ylim(0, 100)\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Model Accuracy Comparison\", pad=12)\n",
        "    plt.xticks(rotation=12)\n",
        "    for b, val in zip(bars, accuracies):\n",
        "        h = b.get_height()\n",
        "        plt.text(b.get_x() + b.get_width() / 2, h + 0.8, f\"{val:.1f}%\", ha='center', va='bottom', fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_fname, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"[plot] Saved accuracy chart to: {out_fname}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Main pipeline\n",
        "# ---------------------------\n",
        "def main():\n",
        "    # Load and preprocess\n",
        "    df = load_data(DATA_PATH)\n",
        "    X_scaled, y = preprocess(df)\n",
        "\n",
        "    # Stratified train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split_stratified(X_scaled, y, test_size=0.30, seed=SEED)\n",
        "\n",
        "    # Fast SMOTE on smaller subset for prototyping\n",
        "    X_res, y_res, X_train_small, y_train_small = fast_smote_subset(X_train, y_train, maj_down_to=MAJORITY_DOWN_TO)\n",
        "\n",
        "    # PCA transform\n",
        "    pca_model, X_res_pca, X_test_pca = apply_pca(X_res, X_test, n_components=PCA_COMPONENTS)\n",
        "\n",
        "    # Train supervised base models\n",
        "    rf, lr, gb = train_supervised_models(X_res_pca, y_res)\n",
        "\n",
        "    # Train AE on majority class (from small training subset)\n",
        "    X_train_major = X_train_small[y_train_small == 0]\n",
        "    X_train_major_pca = pca_model.transform(X_train_major)   # PCA of majority for AE training\n",
        "    ae = build_and_train_autoencoder(X_train_major_pca, input_dim=X_res_pca.shape[1])\n",
        "\n",
        "    # Train isolation forest on majority PCA\n",
        "    iso = IsolationForest(n_estimators=ISO_ESTIMATORS, contamination=0.001, random_state=SEED, n_jobs=-1)\n",
        "    iso.fit(X_train_major_pca)\n",
        "    print(\"[iso] IsolationForest trained on majority-class PCA data.\")\n",
        "\n",
        "    # Compute anomaly scores for test set\n",
        "    anomaly_score, ae_err_norm_test, iso_norm_test = compute_anomaly_scores(ae, iso, X_test_pca)\n",
        "\n",
        "    # Supervised models' probabilities for test set\n",
        "    proba_rf = rf.predict_proba(X_test_pca)[:, 1]\n",
        "    proba_lr = lr.predict_proba(X_test_pca)[:, 1]\n",
        "    proba_gb = gb.predict_proba(X_test_pca)[:, 1]\n",
        "\n",
        "    # Build OOF meta-features (for meta-train)\n",
        "    meta_X_train = build_oof_meta_features(X_res_pca, y_res, ae, iso, folds=STACKING_FOLDS)\n",
        "\n",
        "    # Train meta-classifier on OOF features\n",
        "    meta_clf = LogisticRegression(max_iter=1000, random_state=SEED)\n",
        "    meta_clf.fit(meta_X_train, y_res)\n",
        "    print(\"[meta] Meta-classifier trained on OOF features.\")\n",
        "\n",
        "    # Meta-probabilities for test set (features: rf_prob, lr_prob, gb_prob, anomaly)\n",
        "    meta_X_test = np.vstack([proba_rf, proba_lr, proba_gb, anomaly_score]).T\n",
        "    meta_proba = meta_clf.predict_proba(meta_X_test)[:, 1]\n",
        "\n",
        "    # Calibrate threshold to maximize F1 on small holdout from meta-train\n",
        "    chosen_threshold = calibrate_threshold(meta_X_train, y_res)\n",
        "\n",
        "    # Final predicted labels from hybrid model\n",
        "    y_pred_hybrid = (meta_proba >= chosen_threshold).astype(int)\n",
        "\n",
        "    # Build predictions & scores dicts for evaluation table\n",
        "    model_preds = {\n",
        "        \"Logistic Regression\": (proba_lr >= 0.5).astype(int),   # simple baseline\n",
        "        \"Decision Tree\": None,  # will be set after training a DT baseline below\n",
        "        \"Random Forest\": (proba_rf >= 0.5).astype(int),\n",
        "        \"Logistic (ensemble)\": (proba_lr >= 0.5).astype(int),\n",
        "        \"Autoencoder (Unsuper.)\": (anomaly_score >= np.percentile(anomaly_score, 99)).astype(int),\n",
        "        \"Isolation Forest\": (iso_norm_test >= np.percentile(iso_norm_test, 99)).astype(int),\n",
        "        \"Proposed Hybrid Model\": y_pred_hybrid\n",
        "    }\n",
        "\n",
        "    model_scores = {\n",
        "        \"Logistic Regression\": proba_lr,\n",
        "        \"Decision Tree\": None,  # fill below\n",
        "        \"Random Forest\": proba_rf,\n",
        "        \"Logistic (ensemble)\": proba_lr,\n",
        "        \"Autoencoder (Unsuper.)\": anomaly_score,\n",
        "        \"Isolation Forest\": iso_norm_test,\n",
        "        \"Proposed Hybrid Model\": meta_proba\n",
        "    }\n",
        "\n",
        "    # Train Decision Tree baseline on resampled PCA training data\n",
        "    dt = DecisionTreeClassifier(random_state=SEED)\n",
        "    dt.fit(X_res_pca, y_res)\n",
        "    p_dt = dt.predict_proba(X_test_pca)[:, 1]\n",
        "    model_preds[\"Decision Tree\"] = (p_dt >= 0.5).astype(int)\n",
        "    model_scores[\"Decision Tree\"] = p_dt\n",
        "\n",
        "    # Compute metrics table\n",
        "    res_df = compute_metrics_table(y_test.values,\n",
        "                                   {k: v for k, v in model_preds.items()},\n",
        "                                   {k: v for k, v in model_scores.items()})\n",
        "\n",
        "    # Print & save results\n",
        "    pd.set_option('display.precision', 2)\n",
        "    print(\"\\n[results] Comparative Results:\\n\", res_df)\n",
        "    res_df.to_csv(OUTPUT_CSV)\n",
        "    print(f\"[results] Saved metrics CSV to: {OUTPUT_CSV}\")\n",
        "\n",
        "    # Plot accuracy chart\n",
        "    plot_accuracy_chart(res_df, out_fname=OUTPUT_PLOT)\n",
        "\n",
        "    print(\"[done] Pipeline finished successfully.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhAybhDhNkWk",
        "outputId": "6265bb68-7422-4a19-de26-0f96d04bdce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load_data] Loaded dataset with shape: (284807, 31)\n",
            "[preprocess] Completed standard scaling for 'Amount' and 'Time'.\n",
            "[split] Train shape: (199364, 30)  Test shape: (85443, 30)\n",
            "[split] Train class distribution:\n",
            " Class\n",
            "0    199020\n",
            "1       344\n",
            "Name: count, dtype: int64\n",
            "[fast_smote] Downsampled majority from 199020 to 20000 for prototyping.\n",
            "[fast_smote] After SMOTE: class counts = [20000 20000]\n",
            "[pca] Applied PCA: components=10, explained_variance_ratio_sum=0.962\n",
            "[train_supervised] Training Random Forest, Logistic Regression, Gradient Boosting...\n",
            "[train_supervised] Supervised models trained.\n",
            "[autoencoder] Training AE on majority-class PCA data...\n",
            "Epoch 1/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 10.8541\n",
            "Epoch 2/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.6069\n",
            "Epoch 3/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.9019\n",
            "Epoch 4/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1031\n",
            "Epoch 5/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.0458\n",
            "Epoch 6/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.6996\n",
            "Epoch 7/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2033\n",
            "Epoch 8/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9812\n",
            "Epoch 9/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2883\n",
            "Epoch 10/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9822\n",
            "Epoch 11/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8622\n",
            "Epoch 12/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8170\n",
            "Epoch 13/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7991\n",
            "Epoch 14/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7907\n",
            "Epoch 15/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7856\n",
            "Epoch 16/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7820\n",
            "Epoch 17/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7791\n",
            "Epoch 18/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7767\n",
            "Epoch 19/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7746\n",
            "Epoch 20/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7729\n",
            "Epoch 21/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7714\n",
            "Epoch 22/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7701\n",
            "Epoch 23/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7690\n",
            "Epoch 24/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7680\n",
            "Epoch 25/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7671\n",
            "Epoch 26/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7663\n",
            "Epoch 27/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7655\n",
            "Epoch 28/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7648\n",
            "Epoch 29/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7642\n",
            "Epoch 30/30\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7636\n",
            "[autoencoder] Autoencoder training complete.\n",
            "[iso] IsolationForest trained on majority-class PCA data.\n",
            "\u001b[1m2671/2671\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
            "[oof] Building OOF meta-features using 3-fold CV (this trains small models 3 times)...\n",
            "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    }
  ]
}
